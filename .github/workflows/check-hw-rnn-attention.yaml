name: Check hw rnn attention

on:
  workflow_call:

jobs:
  start-runner-internal:
    uses: fintech-dl-hse/checkhw/.github/workflows/start-runner.yaml@main

  run_autograding:
    needs: start-runner-internal
    name: Autograding
    runs-on: self-hosted
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Checkout classroom files
        uses: actions/checkout@v3
        with:
          sparse-checkout: |
            .github/classroom
          path: checkhw
          repository: fintech-dl-hse/checkhw

      - name: Mkdir .github/classroom
        run: mkdir -p .github/classroom || true

      - name: Restore autograding file
        run: cp checkhw/.github/classroom/autograding-rnn-attention.json .github/classroom/autograding.json

      - name: Extract RNN Attention Class
        run: /data/bin/miniconda3/envs/jupyter-actions/bin/python extract_class_from_notebook.py --notebook "hw_rnn_attention.ipynb" --class_definition "class Seq2SeqRNNAttention(PreTrainedModel):" --out_filename "rnn_attention.py"

      - name: Extract RNN Tokenizer
        run: unzip rnn_tokenizer.zip

      - name: Extract RNN Attention Model
        run: unzip rnn_attention_model.zip

      - name: Autograding
        uses: education/autograding@v1

      - name: rm big files
        if: always()
        run: rm -rf rnn_attention_model.zip rnn_tokenizer.zip rnn_tokenizer rnn_attention_model

      - name: RNN Attention Class
        if: always()
        uses: actions/upload-artifact@v2
        with:
          name: rnn_attention.py
          path: rnn_attention.py
